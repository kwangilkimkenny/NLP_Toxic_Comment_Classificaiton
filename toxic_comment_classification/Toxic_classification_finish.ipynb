{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os, re, csv, math, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers import MaxPool1D, Flatten, Conv1D, GRU, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE='./data/glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE='./data/train.csv'\n",
    "TEST_DATA_FILE='./data/test.csv'\n",
    "MODEL_WEIGHTS_FILE = './toxic_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50 # 단어를 몇 차원으로 임베딩할 것인가(how many dimensions use to embed word?) \n",
    "max_features = 20000 # 몇개의 단어를 주요한 특징으로 볼 것인가(How many words will be the main feature?) \n",
    "maxlen = 100 # 한 comment에서 가져올 수 있는 단어의 최대 갯수(The maximum number of words a comment can get?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Toxic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 처리로 값이 없을 경우에는 _na_ 로 대체함. 값이 없을 경우에 error 발생\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values # comment_text만 가져와서 fillna를 통해 nan를 거른다.\n",
    "                                                                   # Just import comment_text and filter nan through fillna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       "       \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n",
       "       \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       "       ...,\n",
       "       'Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.',\n",
       "       'And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.',\n",
       "       '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측값이 보정된 값 확인으로 커멘트만 배열로 추출되었는지 확인해봄\n",
    "list_sentences_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] # 사용할 컬럼들(to use columns)\n",
    "y = train[list_classes].values # labels of comment_text\n",
    "\n",
    "\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values # Do the same things for test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features) # max_features 만큼의 단어를 Tokenize하기 위한 틀 생성.\n",
    "                                              # Create a frame to Tokenize words as many as max_features.\n",
    "tokenizer.fit_on_texts(list(list_sentences_train)) # just fit\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train) # Tokenize(Transform word into number)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test) # Tokenize(Transform word into number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  688    75     1   126   130   177    29   672  4511 12052  1116    86\n",
      "    331    51  2278 11448    50  6864    15    60  2756   148     7  2937\n",
      "     34   117  1221 15190  2825     4    45    59   244     1   365    31\n",
      "      1    38    27   143    73  3462    89  3085  4583  2273   985]]\n"
     ]
    }
   ],
   "source": [
    "print(np.reshape(list_tokenized_train[0], (1,-1))) # 각 문장(comment)의 단어들을 1(가장 많이 사용된 단어) ~ 20000(2만번째로 많이 사용된 단어) 으로 정수화함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen) # 모든 단어는 각자 다른 길이를 갖고 있으므로 작은 길이의 문장에 \n",
    "                                                         # padding을 해줌으로서 모든 문장의 길이를 maxlen로 맞춰줌\n",
    "                                                         # All comment_texts are different in length, \n",
    "                                                         # so 0 is filled as much as maxlen in small sentences\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen) # do the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  4583,  2273,   985],\n",
       "       [    0,     0,     0, ...,   589,  8377,   182],\n",
       "       [    0,     0,     0, ...,     1,   737,   468],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  3509, 13675,  4528],\n",
       "       [    0,     0,     0, ...,   151,    34,    11],\n",
       "       [    0,     0,     0, ...,  1627,  2056,    88]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t # 모든 문장의 길이를 maxlen으로 통일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52042 , -0.8314  ,  0.49961 ,  1.2893  ,  0.1151  ,  0.057521,\n",
       "       -1.3753  , -0.97313 ,  0.18346 ,  0.47672 , -0.15112 ,  0.35532 ,\n",
       "        0.25912 , -0.77857 ,  0.52181 ,  0.47695 , -1.4251  ,  0.858   ,\n",
       "        0.59821 , -1.0903  ,  0.33574 , -0.60891 ,  0.41742 ,  0.21569 ,\n",
       "       -0.07417 , -0.5822  , -0.4502  ,  0.17253 ,  0.16448 , -0.38413 ,\n",
       "        2.3283  , -0.66682 , -0.58181 ,  0.74389 ,  0.095015, -0.47865 ,\n",
       "       -0.84591 ,  0.38704 ,  0.23693 , -1.5523  ,  0.64802 , -0.16521 ,\n",
       "       -1.4719  , -0.16224 ,  0.79857 ,  0.97391 ,  0.40027 , -0.21912 ,\n",
       "       -0.30938 ,  0.26581 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE)) # glove의 (단어, 벡터)를 가져옴\n",
    "                                                                                     # Get (word, vector) of glove\n",
    "embeddings_index.get(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkwangil/opt/anaconda3/envs/py37Keras/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3357: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values()) # glove의 dict(embeddings_index)에서 벡터만 list 로 변환\n",
    "                                               # Convert vectors of glove's dict(embeddings_index) into list\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std() # glove 벡터들의 평균과 표준편차를 구함\n",
    "                                                   # Obtain the mean and standard deviation of glove vectors\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make embedding maxtirx based GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index # tokenizer에 의해 얻어진 모든 단어들의 배열\n",
    "                                  # an array of all the words obtained by the tokenizer\n",
    "nb_words = min(max_features, len(word_index)) # glove의 영어단어는 너무 많으므로 max_features에 맞출지 word_index에 맞출지 정함\n",
    "                                              # Because glove has too many English words, \n",
    "                                              # it determines whether the number of words matches max_features or word_index\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) # (nb_words X embed_size) 크기의 벡터 필드를 설정\n",
    "                                                                               # Set a vector field of size (nb_words X embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= max_features: \n",
    "        continue # 2만개 이상은 무시(Ignore more than max_feature.)\n",
    "    embedding_vector = embeddings_index.get(word) # comment에서 사용된 단어의 벡터값을 glove에서 가져옴\n",
    "                                                  # The vector value of the word used in comment is taken from glove\n",
    "    \n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector # embedding_matrix에 사용된 단어의 벡터값을 덮어 씌움\n",
    "                                               # Override embedding_matrix with the vector value defined in glove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,)) # X_t가 (maxlen, -1)인지 재확인 및 reshape\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp) # inp를 (max_features X embed_size)의 행렬에 weights의 가중치로 embedding\n",
    "\n",
    "x = SpatialDropout1D(0.2)(x) # 현재 (단어 x embed_size)의 행렬에서 각 문장당 dropout을 수행\n",
    "                                           # 간단히 말하면 몇 단어 빼고서 학습한다는 의미\n",
    "\n",
    "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x) # GRU를 통해 학습\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x) # 그 결과를 CNN에 통과\n",
    "avg_pool = GlobalAveragePooling1D()(x) # CNN의 결과를 AveragePooling하고\n",
    "max_pool = GlobalMaxPooling1D()(x) # CNN의 결과를 MaxPooling한 뒤\n",
    "\n",
    "x = concatenate([avg_pool, max_pool])  # 합쳐서\n",
    "\n",
    "preds = Dense(6, activation=\"sigmoid\")(x) # sigmoid 에 통과한다.\n",
    "\n",
    "# 간단 요약. RNN과 CNN을 통과해 각 문장의 특징들을 추출한 뒤, \n",
    "#그 중 가장 두드러진 특징과 전반적인 특징을 모아서 결론을 냈다.\n",
    "\n",
    "model = Model(inp, preds) # 모델의 input, output설정\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy']) # optimizer는 adam loss는 binary_crossentropy로 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4488/4488 [==============================] - 577s 128ms/step - loss: 0.1077 - accuracy: 0.8519 - val_loss: 0.0614 - val_accuracy: 0.9903\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 2/20\n",
      "4488/4488 [==============================] - 509s 113ms/step - loss: 0.0608 - accuracy: 0.9731 - val_loss: 0.0564 - val_accuracy: 0.9813\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 3/20\n",
      "4488/4488 [==============================] - 483s 108ms/step - loss: 0.0563 - accuracy: 0.9730 - val_loss: 0.0535 - val_accuracy: 0.9752\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 4/20\n",
      "4488/4488 [==============================] - 485s 108ms/step - loss: 0.0536 - accuracy: 0.9674 - val_loss: 0.0521 - val_accuracy: 0.9893\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 5/20\n",
      "4488/4488 [==============================] - 478s 107ms/step - loss: 0.0495 - accuracy: 0.9669 - val_loss: 0.0514 - val_accuracy: 0.9924\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 6/20\n",
      "4488/4488 [==============================] - 480s 107ms/step - loss: 0.0485 - accuracy: 0.9616 - val_loss: 0.0491 - val_accuracy: 0.9204\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 7/20\n",
      "4488/4488 [==============================] - 483s 108ms/step - loss: 0.0467 - accuracy: 0.9522 - val_loss: 0.0486 - val_accuracy: 0.9490\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 8/20\n",
      "4488/4488 [==============================] - 474s 106ms/step - loss: 0.0463 - accuracy: 0.9490 - val_loss: 0.0482 - val_accuracy: 0.9789\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 9/20\n",
      "4488/4488 [==============================] - 519s 116ms/step - loss: 0.0441 - accuracy: 0.9531 - val_loss: 0.0469 - val_accuracy: 0.9868\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 10/20\n",
      "4488/4488 [==============================] - 596s 133ms/step - loss: 0.0439 - accuracy: 0.9462 - val_loss: 0.0472 - val_accuracy: 0.9892\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 11/20\n",
      "4488/4488 [==============================] - 600s 134ms/step - loss: 0.0429 - accuracy: 0.9617 - val_loss: 0.0471 - val_accuracy: 0.9655\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 12/20\n",
      "4488/4488 [==============================] - 619s 138ms/step - loss: 0.0419 - accuracy: 0.9356 - val_loss: 0.0461 - val_accuracy: 0.9677\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 13/20\n",
      "4488/4488 [==============================] - 635s 141ms/step - loss: 0.0414 - accuracy: 0.9577 - val_loss: 0.0467 - val_accuracy: 0.9420\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 14/20\n",
      "4488/4488 [==============================] - 537s 120ms/step - loss: 0.0399 - accuracy: 0.9488 - val_loss: 0.0455 - val_accuracy: 0.9505\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 15/20\n",
      "4488/4488 [==============================] - 544s 121ms/step - loss: 0.0403 - accuracy: 0.9438 - val_loss: 0.0454 - val_accuracy: 0.9471\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 16/20\n",
      "4488/4488 [==============================] - 569s 127ms/step - loss: 0.0390 - accuracy: 0.9361 - val_loss: 0.0457 - val_accuracy: 0.9752\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 17/20\n",
      "4488/4488 [==============================] - 569s 127ms/step - loss: 0.0382 - accuracy: 0.9365 - val_loss: 0.0452 - val_accuracy: 0.9554\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 18/20\n",
      "4488/4488 [==============================] - 585s 130ms/step - loss: 0.0374 - accuracy: 0.9379 - val_loss: 0.0457 - val_accuracy: 0.9729\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 19/20\n",
      "4488/4488 [==============================] - 575s 128ms/step - loss: 0.0372 - accuracy: 0.9440 - val_loss: 0.0458 - val_accuracy: 0.9479\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 20/20\n",
      "4488/4488 [==============================] - 587s 131ms/step - loss: 0.0369 - accuracy: 0.9126 - val_loss: 0.0457 - val_accuracy: 0.9518\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n"
     ]
    }
   ],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00000001)\n",
    "\n",
    "callbacks = [learning_rate_reduction, # learning_rate를 점차 감소시킴으로서 최적값에 접근하는 방식\n",
    "                                                       # approach to get optimal value by gradually decreasing learning_rate\n",
    "             EarlyStopping('val_loss', patience=3), # val_loss이 최적값에서 멀어지는 순간 epoch가 남아도 학습 중지\n",
    "                                                                       # If val_loss deviates from the optimal value, \n",
    "                                                                       # learning stops even if epoch remains.\n",
    "             ModelCheckpoint(MODEL_WEIGHTS_FILE, save_best_only=True)] # 모델을 학습시키면서 지금까지 등장했던 최적의 weight들을 항상 저장한다.\n",
    "                                                                                                            # Always store the optimal weights that have been shown so far \n",
    "                                                                                                            # while learning the model\n",
    "\n",
    "history = model.fit(X_t, y, batch_size=32, epochs=20, validation_split=0.1, callbacks=callbacks); # 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 55s 366ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1) # model에 test data를 넣고 예측\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv') # 예측값을 저장할 csv파일\n",
    "sample_submission[list_classes] = y_test # csv에 저장할 값을 설정\n",
    "sample_submission.to_csv('./data/submission.csv', index=False) # csv파일에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make graph that compare loss and val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                     'training': [ math.sqrt(loss) for loss in history.history['loss'] ],\n",
    "                     'validation': [ math.sqrt(loss) for loss in history.history['val_loss'] ]})\n",
    "ax = loss.loc[:,:].plot(x='epoch', grid=True)\n",
    "ax.set_ylabel(\"root mean squared error\")\n",
    "ax.set_ylim([0.15,0.3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습한 모델로 입력값 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>== Throw from out field to home plate. == \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text\n",
       "0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3       00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4       00017695ad8997eb          I don't anonymously edit articles at all.\n",
       "...                  ...                                                ...\n",
       "153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n",
       "153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n",
       "153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n",
       "\n",
       "[153164 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATA_FILE='./data/test.csv'\n",
    "\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Yo bitch Ja Rule is more succesful then you'll...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test[:1][\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values # Do the same things for test_data\n",
    "\n",
    "list_sentences_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize comment_text\n",
    "tokenizer = Tokenizer(num_words=max_features) # max_features 만큼의 단어를 Tokenize하기 위한 틀 생성.\n",
    "                                              # Create a frame to Tokenize words as many as max_features.\n",
    "\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test) # Tokenize(Transform word into number)\n",
    "\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen) # 모든 단어는 각자 다른 길이를 갖고 있으므로 작은 길이의 문장에 \n",
    "                                                         # padding을 해줌으로서 모든 문장의 길이를 maxlen로 맞춰줌\n",
    "                                                         # All comment_texts are different in length, \n",
    "                                                         # so 0 is filled as much as maxlen in small sentences\n",
    "            \n",
    "X_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 넣어서 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize comment_text\n",
    "tokenizer = Tokenizer(num_words=max_features) # max_features 만큼의 단어를 Tokenize하기 위한 틀 생성.\n",
    "                                              # Create a frame to Tokenize words as many as max_features.\n",
    "\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(input_sentence) # Tokenize(Transform word into number)\n",
    "\n",
    "X_te_ = pad_sequences(list_tokenized_test, maxlen=maxlen) # 모든 단어는 각자 다른 길이를 갖고 있으므로 작은 길이의 문장에 \n",
    "                                                         # padding을 해줌으로서 모든 문장의 길이를 maxlen로 맞춰줌\n",
    "                                                         # All comment_texts are different in length, \n",
    "                                                         # so 0 is filled as much as maxlen in small sentences\n",
    "            \n",
    "X_te_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 174ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06256434, 0.00492993, 0.02044776, 0.00203806, 0.01044104,\n",
       "        0.00120094],\n",
       "       [0.06256434, 0.00492993, 0.02044776, 0.00203806, 0.01044104,\n",
       "        0.00120094],\n",
       "       [0.06256434, 0.00492993, 0.02044776, 0.00203806, 0.01044104,\n",
       "        0.00120094],\n",
       "       ...,\n",
       "       [0.06256428, 0.00492987, 0.02044776, 0.00203806, 0.01044104,\n",
       "        0.00120088],\n",
       "       [0.06256428, 0.00492987, 0.02044776, 0.00203806, 0.01044104,\n",
       "        0.00120088],\n",
       "       [0.06256428, 0.00492993, 0.02044776, 0.00203806, 0.01044104,\n",
       "        0.00120088]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_ = model.predict([X_te_], batch_size=1024, verbose=1) # model에 test data를 넣고 예측\n",
    "y_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_of_submission = pd.read_csv('./data/result_of_submission.csv') # 예측값을 저장할 csv파일\n",
    "result_of_submission[list_classes] = y_test_ # csv에 저장할 값을 설정\n",
    "result_of_submission.to_csv('./data/submission.csv', index=False) # csv파일에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id\\ttoxic\\tsevere_toxic\\tobscene\\tthreat\\tinsult\\tidentity_hate</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062564</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id\\ttoxic\\tsevere_toxic\\tobscene\\tthreat\\tinsult\\tidentity_hate  \\\n",
       "0                                                  NaN                 \n",
       "1                                                  NaN                 \n",
       "2                                                  NaN                 \n",
       "3                                                  NaN                 \n",
       "4                                                  NaN                 \n",
       "..                                                 ...                 \n",
       "362                                                NaN                 \n",
       "363                                                NaN                 \n",
       "364                                                NaN                 \n",
       "365                                                NaN                 \n",
       "366                                                NaN                 \n",
       "\n",
       "        toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0    0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "1    0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "2    0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "3    0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "4    0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "..        ...           ...       ...       ...       ...            ...  \n",
       "362  0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "363  0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "364  0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "365  0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "366  0.062564       0.00493  0.020448  0.002038  0.010441       0.001201  \n",
       "\n",
       "[367 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_fin = pd.read_csv('./data/submission.csv')\n",
    "result_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.062564\n",
       "1      0.062564\n",
       "2      0.062564\n",
       "3      0.062564\n",
       "4      0.062564\n",
       "         ...   \n",
       "362    0.062564\n",
       "363    0.062564\n",
       "364    0.062564\n",
       "365    0.062564\n",
       "366    0.062564\n",
       "Length: 367, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val = result_fin.max(axis=1)\n",
    "result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625643427268036"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic',\n",
       "       'toxic', 'toxic',\n",
       "       ...\n",
       "       'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic', 'toxic',\n",
       "       'toxic', 'toxic'],\n",
       "      dtype='object', length=367)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val.index = result_fin.idxmax(axis=1)\n",
    "result_val.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.base.Index"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic',\n",
       " 'toxic']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추출한 값을 리스트로 만들기\n",
    "final_result = list(result_val.index)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def most_common(input_list):\n",
    "    \"\"\"가장 많이 나오는 값을 추출한다.\"\"\"\n",
    "    assert isinstance(input_list, list), 'Must be a list type'\n",
    "    if len(input_list) == 0: return None\n",
    "    return Counter(input_list).most_common(n=1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toxic'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최빈값 구하기\n",
    "most_common(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행 기능을 하나의 함수로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def most_common(input_list):\n",
    "    \"\"\"가장 많이 나오는 값을 추출한다.\"\"\"\n",
    "    assert isinstance(input_list, list), 'Must be a list type'\n",
    "    if len(input_list) == 0: return None\n",
    "    return Counter(input_list).most_common(n=1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('toxic_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_comment(input_sent):\n",
    "    # Tokenize comment_text\n",
    "    tokenizer = Tokenizer(num_words=max_features) # max_features 만큼의 단어를 Tokenize하기 위한 틀 생성.\n",
    "                                                  # Create a frame to Tokenize words as many as max_features.\n",
    "\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(input_sentence) # Tokenize(Transform word into number)\n",
    "\n",
    "    X_te_ = pad_sequences(list_tokenized_test, maxlen=maxlen) # 모든 단어는 각자 다른 길이를 갖고 있으므로 작은 길이의 문장에 \n",
    "                                                             # padding을 해줌으로서 모든 문장의 길이를 maxlen로 맞춰줌\n",
    "                                                             # All comment_texts are different in length, \n",
    "                                                             # so 0 is filled as much as maxlen in small sentences\n",
    "                \n",
    "    y_test_ = model.predict([X_te_], batch_size=1024, verbose=1) # model에 test data를 넣고 예측\n",
    "    \n",
    "    result_of_submission = pd.read_csv('./data/result_of_submission.csv') # 예측값을 저장할 csv파일\n",
    "    result_of_submission[list_classes] = y_test_ # csv에 저장할 값을 설정\n",
    "    result_val = result_of_submission.max(axis=1)\n",
    "    result_val.mean()\n",
    "    result_val.index = result_fin.idxmax(axis=1)\n",
    "    final_result = list(result_val.index)\n",
    "    # 최빈값 구하기\n",
    "    result__ = most_common(final_result)\n",
    "    return result__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_test = \"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 575ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'toxic'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_finfin = check_comment(input_sentence_test)\n",
    "re_finfin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
